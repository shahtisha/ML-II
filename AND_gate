import numpy as np

def step_func(y_in):
  if y_in < 0:
    return 0
  else:
    return 1

def perceptron(x,w,b):
  y_in = np.dot(x, w)+b
  y_pred = step_func(y_in)
  return y_pred

X = ([0,0], [0,1], [1,0], [1,1])

def and_func(x):
  w = np.array([1,1])
  b = -2
  return perceptron(x, w, b)

print("AND({}, {}) = {}".format(0,0,and_func(X[0])))
print("AND({}, {}) = {}".format(0,1,and_func(X[1])))
print("AND({}, {}) = {}".format(1,0,and_func(X[2])))
print("AND({}, {}) = {}".format(1,1,and_func(X[3])))

learning_rate = 0.1
weights = np.array([0.5, 0.5])
bias = 0
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y_out = np.array([0,0,0,1])

for epoch in range(100):
  for i in range(len(X)):
    y_in = np.dot(weights, X[i])+bias
    y_pred = step_func(y_in)
    error = y_out[i] - y_pred
    weights += learning_rate*error*X[i]
    bias += learning_rate*error
print("Final weights: ",weights)
print("Final bias: ",bias)

for i in range(len(X)):
  y_in = np.dot(weights, X[i]) + bias
  y_pred = step_func(y_in)
  print("Input: ",X[i], "Output: ", y_pred)
